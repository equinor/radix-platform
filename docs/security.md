## Edge security / HTTPS / TLS Termination

### SSL

A SSL certificate is a way for a server to prove it's identity to a client. Certificates are also used to establish encrypted communication between a (web) server and client (browser).

Historically, one single server (host) had a single and corresponding hostname which was saved as a DNS record to form a fully qualified domain name (FQDN). So the logical thing was to issue SSL certificates for a FQDN meant to be used on the corresponding server.

### Name based virtual hosts

In the late 90-ties hosting different websites on the same server (using name-based virtual hosts) became normal and the 1:1 relation between a server and hostnames were broken up. About 7-10 years later an extension to TLS were added which allowed a single certificate to be used for several different hostnames. This extension is called SNI, Server Name Indication.

Traditionally aquiring SSL certificates was costly since it both cost money as well as effort to buy and install the certificates manually. SNI and wildcard certificates have also been very costly.

### Let's Encrypt

This changed in 2016 when Let's Encrypt was formed. "Let's Encrypt is a certificate authority that provides free X.509 certificates for Transport Layer Security (TLS) encryption via an automated process designed to eliminate the hitherto complex process of manual creation, validation, signing, installation, and renewal of certificates for secure websites."

This made it much easier and cheaper to secure highly dynamic environments where hostnames and certificates needs to change frequently and with low friction.

### Let's Encrypt Automation

There is still some complexity to automating the generation of certificates with Let's Encrypt since the ownership of a domain first needs to be verified. This can be done by updating DNS records with a challenge-response generated by Let's Encrypt centrally or starting a server on a domainname that answers the challenge-response.

This makes the most logic place of setting up SSL certificate automation on the web server or proxy server that is terminating SSL/TLS. This server can answer the challenge-response as well as install and start using the generated certificates directly. Caddy is a webserver that does this natively. Tr√¶fik is a reverse proxy that does this natively. lua-resty-auto-ssl does this for OpenResty(nginx) webserver/proxy. 

There is also a native golang package in the works that wraps a regular HTTP server to request and use Let's Encrypt certificates automatically (https://godoc.org/golang.org/x/crypto/acme/autocert).

openshift-acme (https://github.com/tnozicka/openshift-acme) is a OpenShift(Kubernetes) controller that automatically manages certificates based on Kubernetes resources and annotations. Ingress and Kubernetes support is on the roadmap. 

cert-manager is a Kubernetes "addon" distributed as a Helm Chart that can watch Ingress resources and request corresponding certificates for the hostnames. cert-manager seems to have integrations to nginx-ingress controller for distribution of the certificates to the actual proxy servers doing SSL termination.

### Wildcard certificates

A third type of certificate is the wildcard certificate. It can be used to prove identity for any current and future subdomains (at a single level, for example *.radix.equinor.com will match someapp.radix.equinor.com but NOT api.someapp.radix.equinor.com).

Let's Encrypt did not issue wildcard certificates until march 2018 which is partly the reason so many tools got created for dynamic issuing and renewal of certificates.

The pros and cons with using a wildcard certificate depends on the threat model. If the private key of a wildcard certificate is compromised, an attacker can impersonate any domainname the wildcard is valid for. In many setups that is in effect any server/webapplication in an organization.

If the architecture is so that every webapplication is responsible for terminating their own SSL that would require every application to have a copy of the wildcard private key. And the resulting attack surface would be very large.

But if there is a central component doing SSL termination on behalf of many or most of the applications, this component would either have the private keys for all the webapplications it would be protecting or the private key for the wildcard certificate. The attack surfaces would be similar.

A process/software responsible for requesting a certificate, either on demand or from a list of gathered domainnames, and answering ownership challenges using DNS or direct connection would also expose an attack surface where compromise also could lead to generation or exposure of arbitrary certificates and maybe upstream DNS manipulation.




## Secrets in Kubernetes
The Design Proposal for secrets in Kubernetes is here: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/auth/secrets.md

Which says:

"It is assumed that node and master are secure and that compromising their security could also compromise secrets:
If a node is compromised, the only secrets that could potentially be exposed should be the secrets belonging to containers scheduled onto it If the master is compromised, all secrets in the cluster may be exposed"

Kubernetes secrets are not encrypted, only encoded (base64).

Secrets can be consumed by a pod either injected as environment variables or mounted into the pod as a file. Secrets live within a namespace and can only be accessed from pods within that namespace.

So unsolved problems are mostly encryption and access control and audit logs and key rolling for the secrets.

There are not that many tools for extending the builtin secret stuff.
  * Hashicorp Vault - Encrypted storage. Secret/key generation. Encryption as a service. Leasing, renewal and revocation of secrets.
  * Docker Vault
  * Square KeyWhiz
  * RedHat KeyCloack
  * Bitnami Sealed Secrets

### Conclusion

At this point in time we do not have a defined threat model and have concluded that the builtin secrets of Kubernetes will suffice until we discover any specific needs or threats that need to be mitigated.

## Secrets during Docker build time

Currently there are few good ways of handling security and secrets during "docker build". There are lots of hacks to pass secrets to a docker build context, all with considerable drawbacks:
  - Use --build-args
    - Discouraged as these can be seen in the final image metadata
  - Copy in a file and use multi-stage builds
    - Best way today. Risk for us since any Dockerfile provided by a developer can read the secret file which might contain a master key to access all Statoil private repos and export it during build.
  - Mount a ssh agent socket into the build context.
    - Only works for ssh, platform specific, how to get working with Jenkins or other build tools?
  - Have a key-server (netcat/nc) outside and have the build context connect and pipe the output into the process inside docker build.
    - Lots of potential problems and incompatibilities and security weaknesses.
  - Environment variables.
  - ++++

Some history from the Docker project:

January 2015 - **The Docker Vault Proposal** (https://github.com/moby/moby/issues/10310)
6 months of discussion. No solution. Discussion continued in **Secrets: write-up best practices, do's and don'ts, roadmap** (https://github.com/moby/moby/issues/13490). Issue is still open and unsolved. One attempt to fix the security of --build-args were rejected (https://github.com/moby/moby/pull/36443#issuecomment-369845823). The only solutions officially supported by the Docker devs are:
  - Pull in all external dependencies from private repos BEFORE running docker build, basically outside docker.
  - COPY in a file in a multi stage build.

The Azure Draft team are discussing whether to support other tools for building docker images to fix this specific problem (https://github.com/Azure/draft/issues/564). One such tool to consider is https://github.com/cloud66/habitus .